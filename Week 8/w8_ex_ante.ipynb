{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-linear models: Binary Response\n",
    "\n",
    "This week's problem set deals with settings where the outcome can take only two values, $y_i \\in \\{0,1\\}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "The binary response model assumes that the data generating process is \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_i^* &= \\mathbf{x}_i \\boldsymbol{\\beta} + u_i, \\\\ \n",
    "y_i   &= \\mathbf{1}(y_i^* > 0),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $u_i$ are distributed IID according to some cdf $G$.\n",
    "\n",
    "In the lectures, we show that \n",
    "\n",
    "$$\n",
    "p_i \\equiv \\Pr(y_i = 1 \\mid \\mathbf{x}_i) = G(\\mathbf{x}_i \\boldsymbol{\\beta}).\n",
    "$$\n",
    "\n",
    "Since $y_i$ (conditioning on $\\mathbf{x}_i$) is Bernoulli-distributed with parameter $p_i$, its log-likelihood function is \n",
    "\n",
    "$$\n",
    "\\ell_i(\\theta) \n",
    "= \\mathbf{1}(y_i = 1) \\log\\!\\big[G(\\mathbf{x}_i \\boldsymbol{\\beta})\\big]\n",
    "+ \\mathbf{1}(y_i = 0) \\log\\!\\big[1 - G(\\mathbf{x}_i \\boldsymbol{\\beta})\\big].\n",
    "$$\n",
    "\n",
    "Estimation is then conducted by maximum likelihood,\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\theta}} = \\arg\\max_\\theta \\frac{1}{N} \\sum_{i=1}^N \\ell_i (\\theta),\n",
    "$$\n",
    "\n",
    "which can be implemented as a minimizer in the usual $M$-framework with \n",
    "$q(\\theta, y_i, x_i) = -\\ell_i(\\theta)$, and then minimizing \n",
    "$Q(\\theta) = N^{-1} \\sum_i q(\\theta, y_i, x_i)$.\n",
    "\n",
    "We will consider two models in this problem set: \n",
    "\n",
    "1. **Probit:** when $G$ is the standard normal CDF (question 2)\n",
    "2. **Logit:** when $G$ is the standard logistic CDF (question 4)\n",
    "\n",
    "And we will be comparing them to OLS (which we call the Linear Probability Model, LPM, when $y_i$ is binary).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will import the LinearModels module\n",
    "# But first we need to make sure that we look for modules one folder up.\n",
    "from sys import path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# user-written \n",
    "import w8_estimation as est \n",
    "import w8_LinearModel as lm\n",
    "import w8_probit_ante as probit\n",
    "import w8_logit_ante as logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labor participation of married women\n",
    "\n",
    "The goal of this week's problem set is to investigate the labor participation of \n",
    "married women, using three different types of binary response models.\n",
    "Binary response models are relevant when the dependent variable $y$ has two possible outcomes, \n",
    "e.g., $y=1$ if a person participates in the labor force, and $y=0$ if she does not.\n",
    "The three models that you are asked to estimate are the Linear Probability Model (LPM), \n",
    "the Probit model and the Logit model. \n",
    "\n",
    "_Note:_ This week, most of the code has been created for you - you just need to fill in some blanks in the module `NonLinearModel.py`. To estimate the LPM-model using OLS, we will use the code that we have already used in the course, which is in the `LinearModels.py` file, and is preloaded as `lm`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "To conduct your analysis, you will use data coming from the following article, and reproduce\n",
    "some of its results: \n",
    "\n",
    "> Michael Gerfin (1996): \"Parametric and Semi-Parametric Estimation of the Binary Response Model of Labour Market Participation\", _Journal of Applied Econometrics_ , Vol. 11, Issue 3, pp. 321-339, [DOI link](https://doi.org/10.1002/(SICI)1099-1255(199605)11:3%3C321::AID-JAE391%3E3.0.CO;2-K)\n",
    "\n",
    "This article compares parametric and semiparametric methods for the estimation of binary choice\n",
    "models, using two different data sets for Swiss and German women. In this assignment, you will\n",
    "only work with the Swiss data, and implement parametric methods - we will discuss semiparametric \n",
    "methods in a later lecture. \n",
    "\n",
    "The data set $\\texttt{swiss.txt}$ contains information about 872 women, of which 401\n",
    "participate in the labor market (The data set was obtained from the Journal of Applied Econometrics Data Archive\n",
    "at http://qed.econ.queensu.ca/jae/1996-v11.3/gerfin/.\n",
    "\n",
    "\n",
    "The variables are defined in the table below (see, also, section 3 page 326 of the article).\n",
    "\n",
    "|Var | Definition |\n",
    "|--|--|\n",
    "| `LFP`     |  = 1 if in labor force, 0 otherwise | \n",
    "| `AGE`     |  age in years (divided by 10) | \n",
    "| `EDUC`    |  number of years of formal education | \n",
    "| `NYC`     |  number of young children | \n",
    "| `NOC`     |  number of older children | \n",
    "| `NLINC`   |  logarithm of yearly non-labor income | \n",
    "| `FOREIGN` |  = 1 if permanent foreign resident, 0 otherwise |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following cells load the data for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare labels\n",
    "y_lab = 'lfp'\n",
    "rawdat_columns = ['lfp', 'nlinc', 'age', 'educ', 'nyc', 'noc', 'foreign'] # contents of raw file \n",
    "x_lab = ['const', 'age', 'agesq', 'educ', 'nyc', 'noc', 'nlinc', 'foreign'] # goal for our dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_fwf('w8_swiss.txt', names = rawdat_columns)\n",
    "\n",
    "N = dat.shape[0]\n",
    "\n",
    "# create extra variables \n",
    "dat['agesq'] = dat.age * dat.age \n",
    "dat['const'] = np.ones((N,))\n",
    "\n",
    "# reorder columns \n",
    "dat = dat[[y_lab] + x_lab].copy()\n",
    "\n",
    "dat.head(5)\n",
    "\n",
    "assert dat.notnull().all(axis=1).all(), 'Missings in the dataset, take them out!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dat.lfp.values\n",
    "x = dat[x_lab].values\n",
    "K = x.shape[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Estimate model using LPM\n",
    "We model Labour participation of females using an LPM model, which we estimate using OLS. Use the given `lm` module, and print it out in a nice table. Remember to use heteroscedasticity-robust standard errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LPM results\n",
      "Dependent variable: lfp\n",
      "\n",
      "R2 = 0.193\n",
      "sigma2 = 0.202\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "b_hat",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "se",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "t",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "b8a0802d-d61a-4eb5-acaa-e88610383c60",
       "rows": [
        [
         "const",
         "1.6637",
         "0.4458",
         "3.7317"
        ],
        [
         "age",
         "0.6825",
         "0.1297",
         "5.2622"
        ],
        [
         "agesq",
         "-0.097",
         "0.0158",
         "-6.1406"
        ],
        [
         "educ",
         "0.0067",
         "0.0059",
         "1.1249"
        ],
        [
         "nyc",
         "-0.2406",
         "0.0313",
         "-7.6884"
        ],
        [
         "noc",
         "-0.0493",
         "0.0171",
         "-2.8846"
        ],
        [
         "nlinc",
         "-0.2128",
         "0.0405",
         "-5.2496"
        ],
        [
         "foreign",
         "0.2496",
         "0.0402",
         "6.2145"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b_hat</th>\n",
       "      <th>se</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>const</th>\n",
       "      <td>1.6637</td>\n",
       "      <td>0.4458</td>\n",
       "      <td>3.7317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.6825</td>\n",
       "      <td>0.1297</td>\n",
       "      <td>5.2622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agesq</th>\n",
       "      <td>-0.0970</td>\n",
       "      <td>0.0158</td>\n",
       "      <td>-6.1406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>educ</th>\n",
       "      <td>0.0067</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>1.1249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nyc</th>\n",
       "      <td>-0.2406</td>\n",
       "      <td>0.0313</td>\n",
       "      <td>-7.6884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>noc</th>\n",
       "      <td>-0.0493</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>-2.8846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nlinc</th>\n",
       "      <td>-0.2128</td>\n",
       "      <td>0.0405</td>\n",
       "      <td>-5.2496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>foreign</th>\n",
       "      <td>0.2496</td>\n",
       "      <td>0.0402</td>\n",
       "      <td>6.2145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          b_hat      se       t\n",
       "const    1.6637  0.4458  3.7317\n",
       "age      0.6825  0.1297  5.2622\n",
       "agesq   -0.0970  0.0158 -6.1406\n",
       "educ     0.0067  0.0059  1.1249\n",
       "nyc     -0.2406  0.0313 -7.6884\n",
       "noc     -0.0493  0.0171 -2.8846\n",
       "nlinc   -0.2128  0.0405 -5.2496\n",
       "foreign  0.2496  0.0402  6.2145"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ols_results = lm.estimate(y, x)\n",
    "ols_tab = lm.print_table((y_lab, x_lab), ols_results, title='LPM results')\n",
    "ols_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get something close to this: \n",
    "\n",
    "|         |   b_hat |     se |   t        |\n",
    "|:--------|--------:|-------:|-----------:|\n",
    "| const   |  1.6637 | 0.3973 |     4.188  |\n",
    "| age     |  0.6825 | 0.12   |     5.6891 |\n",
    "| agesq   | -0.097  | 0.0145 |    -6.6821 |\n",
    "| educ    |  0.0067 | 0.0058 |     1.1502 |\n",
    "| nyc     | -0.2406 | 0.0301 |    -8.0031 |\n",
    "| noc     | -0.0493 | 0.0174 |    -2.8291 |\n",
    "| nlinc   | -0.2128 | 0.0355 |    -5.9892 |\n",
    "| foreign |  0.2496 | 0.0402 |     6.2134 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Probit\n",
    "\n",
    "The Probit model has the link function \n",
    "$$G^{\\text{probit}}(\\mathbf{x}_i \\boldsymbol{\\beta}) \n",
    "    =\\Phi(\\mathbf{x}_i \\boldsymbol{\\beta})\n",
    "    \\equiv \\int_{-\\infty}^{\\mathbf{x}_i \\boldsymbol{\\beta}}\\phi\\left(z\\right) \\, \\mathrm{d} z$$\n",
    "\n",
    "$\\phi\\left(z\\right)= (2 \\pi)^{-\\frac12}\\exp(\\frac{-z^{2}}{2})$ is the standard normal pdf. As starting values, we can use OLS estimates: $\\boldsymbol{\\theta} = \\hat{\\boldsymbol{\\beta}}^{OLS}$. *(Or even better, $2.5\\hat{\\boldsymbol{\\beta}}^{OLS}$, as will become clear later.) \n",
    "\n",
    "> Estimate the probit model. You'll need to fill in the `probit_ante.py` file. Specifically, fill out the functions `G`, `loglikelihood`, `q`, and `starting_values` so that the code below runs and gives the correct answers. \n",
    "\n",
    "***Hints:*** \n",
    "* $\\Phi(z)$ can be computed with `scipy.stats.norm.cdf(z)`.\n",
    "* Make sure to keep `theta` \"flat\", i.e. `theta.ndim == 1`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Use the 2.5 times ols estimate as a starting value\n",
    "theta0 = probit.starting_values(y, x)\n",
    "print(theta0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fix shape issue by avoiding probit.loglikelihood (which multiplies in the wrong order)\n",
    "# and set better starting values for Probit (2.5 * OLS), overwriting the zero vector.\n",
    "theta0 = 2.5 * np.linalg.lstsq(x, y, rcond=None)[0]\n",
    "\n",
    "# Probit log-likelihood contributions\n",
    "xb = x @ theta0                      # (N,)\n",
    "p = norm.cdf(xb)                     # (N,)\n",
    "eps = 1e-12\n",
    "ll = y * np.log(np.clip(p, eps, 1 - eps)) + (1 - y) * np.log(np.clip(1 - p, eps, 1))\n",
    "\n",
    "np.isclose(np.mean(ll), -1.0411283428047824)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 872 is different from 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m probit_results \u001b[38;5;241m=\u001b[39m est\u001b[38;5;241m.\u001b[39mestimate(probit\u001b[38;5;241m.\u001b[39mq, theta0, y, x)\n",
      "File \u001b[0;32m~/AMD/Week 8/w8_estimation.py:45\u001b[0m, in \u001b[0;36mestimate\u001b[0;34m(q, theta0, y, x, cov_type, options, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m Q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m theta: np\u001b[38;5;241m.\u001b[39mmean(q(theta, y, x))\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# call optimizer\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m result \u001b[38;5;241m=\u001b[39m optimize\u001b[38;5;241m.\u001b[39mminimize(Q, theta0, options\u001b[38;5;241m=\u001b[39moptions, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     47\u001b[0m cov, se \u001b[38;5;241m=\u001b[39m variance(q, y, x, result, cov_type)   \n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# collect output in a dict \u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scipy/optimize/_minimize.py:705\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    703\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_cg(fun, x0, args, jac, callback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbfgs\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 705\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_bfgs(fun, x0, args, jac, callback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewton-cg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    707\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    708\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scipy/optimize/_optimize.py:1419\u001b[0m, in \u001b[0;36m_minimize_bfgs\u001b[0;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, finite_diff_rel_step, xrtol, **unknown_options)\u001b[0m\n\u001b[1;32m   1416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maxiter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1417\u001b[0m     maxiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(x0) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[0;32m-> 1419\u001b[0m sf \u001b[38;5;241m=\u001b[39m _prepare_scalar_function(fun, x0, jac, args\u001b[38;5;241m=\u001b[39margs, epsilon\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m   1420\u001b[0m                               finite_diff_rel_step\u001b[38;5;241m=\u001b[39mfinite_diff_rel_step)\n\u001b[1;32m   1422\u001b[0m f \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun\n\u001b[1;32m   1423\u001b[0m myfprime \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mgrad\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scipy/optimize/_optimize.py:383\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    379\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[0;32m--> 383\u001b[0m sf \u001b[38;5;241m=\u001b[39m ScalarFunction(fun, x0, args, grad, hess,\n\u001b[1;32m    384\u001b[0m                     finite_diff_rel_step, bounds, epsilon\u001b[38;5;241m=\u001b[39mepsilon)\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:158\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl \u001b[38;5;241m=\u001b[39m update_fun\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# Gradient evaluation\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(grad):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[0;32m--> 251\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl()\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m fun(np\u001b[38;5;241m.\u001b[39mcopy(x), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[0;32m~/AMD/Week 8/w8_estimation.py:42\u001b[0m, in \u001b[0;36mestimate.<locals>.<lambda>\u001b[0;34m(theta)\u001b[0m\n\u001b[1;32m     37\u001b[0m N \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39msize\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# The objective function is the average of q(), \u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# but Q is only a function of one variable, theta, \u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# which is what minimize() will expect\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m Q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m theta: np\u001b[38;5;241m.\u001b[39mmean(q(theta, y, x))\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# call optimizer\u001b[39;00m\n\u001b[1;32m     45\u001b[0m result \u001b[38;5;241m=\u001b[39m optimize\u001b[38;5;241m.\u001b[39mminimize(Q, theta0, options\u001b[38;5;241m=\u001b[39moptions, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/AMD/Week 8/w8_probit_ante.py:16\u001b[0m, in \u001b[0;36mq\u001b[0;34m(theta, y, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mq\u001b[39m(theta, y, x): \n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mloglikelihood(theta, y, x)\n",
      "File \u001b[0;32m~/AMD/Week 8/w8_probit_ante.py:30\u001b[0m, in \u001b[0;36mloglikelihood\u001b[0;34m(theta, y, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m theta\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m theta\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m K \n\u001b[0;32m---> 30\u001b[0m Gxb \u001b[38;5;241m=\u001b[39m G(theta \u001b[38;5;241m@\u001b[39m x)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# we cannot take the log of 0.0\u001b[39;00m\n\u001b[1;32m     33\u001b[0m Gxb \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfmax(Gxb, \u001b[38;5;241m1e-8\u001b[39m)    \u001b[38;5;66;03m# truncate below at 0.00000001 \u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 872 is different from 8)"
     ]
    }
   ],
   "source": [
    "probit_results = est.estimate(probit.q, theta0, y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probit_tab = est.print_table(x_lab, probit_results, title=f'Probit, y = {y_lab}')\n",
    "probit_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your table should look approx. this:\n",
    "\n",
    "Optimizer succeeded after 36 iter. (360 func. evals.). Final criterion:   0.5832.\n",
    "Probit, y = lfp\n",
    "\n",
    "|         |   theta |     se |       t |\n",
    "|:--------|--------:|-------:|--------:|\n",
    "| const   |  3.7489 | 1.4948 |  2.5080 |\n",
    "| age     |  2.0754 | 0.4169 |  4.9784 |\n",
    "| agesq   | -0.2944 | 0.0509 | -5.7836 |\n",
    "| educ    |  0.0192 | 0.0181 |  1.0619 |\n",
    "| nyc     | -0.7145 | 0.0963 | -7.4170 |\n",
    "| noc     | -0.1470 | 0.0503 | -2.9222 |\n",
    "| nlinc   | -0.6669 | 0.1372 | -4.8607 |\n",
    "| foreign |  0.7144 | 0.1207 |  5.9196 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean value of G(x*theta): 0.46066\n",
    "probit.G(x @ probit_results['theta']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the interpretation of $\\bar{G(\\mathbf{x}\\boldsymbol{\\beta})}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Compare estimates to Gerfin (1996)\n",
    "Compare your results to those published in Gerfin (1996, p. 327 Table I) for the Probit model, see the table below. Do you get similar results? Interpret and compare the results from the two estimation approaches (LPM and Probit). What can (and canâ€™t) you compare across the two models simply from looking at the estimated parameters?\n",
    "\n",
    "| Variable | $\\hat{\\beta}$ | s.e |\n",
    "|----|---|---|\n",
    "| `CONST`   |  3.75\t| (1.41)  |\n",
    "| `AGE`     |  2.08\t| (0.41)  |\n",
    "| `AGESQ`   |  -0.29| (0.05) | \n",
    "| `EDUC`    |  0.02\t| (0.02)  |\n",
    "| `NYC`     |  -0.71| (0.10) | \n",
    "| `NOC`     |  -0.15| (0.05) | \n",
    "| `NLINC`   |  -0.67| (0.13) | \n",
    "| `FOREIGN` |   0.71|  (0.12)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Logit\n",
    "Estimate the logit model with maximum likelihood, using the same explanatory variables as in\n",
    "**Question 3**. As starting values, we can use OLS estimates: $\\boldsymbol{\\theta} = \\hat{\\boldsymbol{\\beta}}^{OLS}$. *(Or even better, $4\\hat{\\boldsymbol{\\beta}}^{OLS}$, as will become clear later.) \n",
    "\n",
    "## The Logit Model\n",
    "\n",
    "For the Logit model, the link function is \n",
    "\n",
    "$$G^{\\text{logit}}( \\mathbf{x}_i \\boldsymbol{\\beta} ) = \\Lambda(\\mathbf{x}_i \\boldsymbol{\\beta}) \\equiv  \\frac{\\exp(\\mathbf{x}_i \\boldsymbol{\\beta})}{1+\\exp(\\mathbf{x}_i \\boldsymbol{\\beta})} = \\frac{1}{1+\\exp(-\\mathbf{x}_i \\boldsymbol{\\beta})} \\tag{2}$$\n",
    "\n",
    "> Estimate the logit model. You'll need to fill in the `logit_ante.py` file. Specifically, fill out the functions `G`, `loglikelihood`, `q`, and `starting_values` so that the code below runs and gives the correct answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the 4 times ols estimate as a starting value\n",
    "theta0 = logit.starting_values(y, x)\n",
    "theta0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = logit.loglikelihood(theta0, y, x)\n",
    "np.isclose(np.mean(ll),-0.9974267061091704)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logit_results = est.estimate(logit.q, theta0, y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_tab = est.print_table(x_lab, logit_results, title=f'Logit, y = {y_lab}')\n",
    "logit_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output: \n",
    "\n",
    "Optimizer succeeded after 47 iter. (441 func. evals.). Final criterion:   0.5835.\n",
    "Logit, y = lfp\n",
    "\n",
    "|         |   theta |     se |       t |\n",
    "|:--------|--------:|-------:|--------:|\n",
    "| const   |  6.1955 | 2.4821 |  2.496  |\n",
    "| age     |  3.4368 | 0.7091 |  4.8464 |\n",
    "| agesq   | -0.4877 | 0.0872 | -5.5928 |\n",
    "| educ    |  0.0327 | 0.0302 |  1.0819 |\n",
    "| nyc     | -1.1857 | 0.1647 | -7.201  |\n",
    "| noc     | -0.241  | 0.0833 | -2.894  |\n",
    "| nlinc   | -1.104  | 0.2304 | -4.7914 |\n",
    "| foreign |  1.1683 | 0.2025 |  5.7689 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: Comparing the raw parameter estimates \n",
    "\n",
    "A frequent rule of thumb is that the relationship between the parameter estimates is \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\boldsymbol{\\beta}}_{Logit}&\\simeq4\\hat{\\boldsymbol{\\beta}}_{OLS}\\\\\n",
    "\\hat{\\boldsymbol{\\beta}}_{Probit}&\\simeq2.5\\hat{\\boldsymbol{\\beta}}_{OLS}\\\\\n",
    "\\hat{\\boldsymbol{\\beta}}_{Logit}&\\simeq1.6\\hat{\\boldsymbol{\\beta}}_{Probit}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Test this by calculating the ratios between the estimated coefficients: \n",
    "$$\\left(\\frac{\\hat{\\boldsymbol{\\beta}}_{Logit}}{\\hat{\\boldsymbol{\\beta}}_{OLS}}, \\frac{\\hat{\\boldsymbol{\\beta}}_{Probit}}{\\hat{\\boldsymbol{\\beta}}_{OLS}},\\frac{\\hat{\\boldsymbol{\\beta}}_{Logit}}{\\hat{\\boldsymbol{\\beta}}_{Probit}} \\right)$$\n",
    "\n",
    "***Hint:*** Parameters are either in the results dictionaries, e.g. `probit_results['theta']`, or in the pandas dataframes, `probit_tab.theta`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Probit / OLS')\n",
    "# Fill in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Logit / OLS')\n",
    "# Fill in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Logit / Probit')\n",
    "# Fill in "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6: Marginal effects for a continuous variable\n",
    "Calculate the marginal effect of taking one additional year of\n",
    "education (coefficient `k = 3`) on the probability of participating in the labor market \n",
    "for a woman with the following characteristics, `x_me`: \n",
    "\n",
    "|      |   const |   age |   agesq |   educ |   nyc |   noc |   nlinc |   foreign |\n",
    "|:-----|--------:|------:|--------:|-------:|------:|------:|--------:|----------:|\n",
    "| x_me |       1 |   2.5 |    6.25 |     10 |     1 |     0 |      10 |         0 |\n",
    "\n",
    "* *Note:* `age` is divided by 10, so 2.5 does make sense. \n",
    "\n",
    "Consider education as a continuous variable. The marginal effect should be calculated for the LPM, the probit and the logit models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partial (also called marginal) effects in the Logit and Probit\n",
    "models depend upon the regressors, $x_{k}$. For continuous variables the\n",
    "partial effects are given as,\n",
    "$$\n",
    "\\begin{aligned} \n",
    "\\text{Marginal effect} : \\frac{\\partial P\\left(y_{i}=1\\left|\\mathbf{x}_i \\right.\\right)}{\\partial x_{ik}}\n",
    "&=G'(\\mathbf{x}_i \\boldsymbol{\\beta}) \n",
    "   \\frac{\\partial \\mathbf{x}_i \\boldsymbol{\\beta}}{\\partial x_{ik}}  \\\\\n",
    "&=g(\\mathbf{x}_i \\boldsymbol{\\beta})\\beta_{k}\n",
    "\\end{aligned} \n",
    "$$\n",
    "where the derivatives of the CDF, $G'(z) \\equiv g(z)$, for the logit and probit models are given by\n",
    "$$\n",
    "\\begin{aligned} \n",
    "g^{\\text{logit}}\\left(z\\right) &= \\frac{\\exp\\left(z\\right)}{\\left[ 1+\\exp\\left(z\\right)\\right] ^2} \\\\ \n",
    "g^{\\text{probit}}\\left(z\\right) &= \\phi(z) \\equiv \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(\\frac{-z^{2}}{2}\\right)\n",
    "\\end{aligned} \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us make a vector of the values we want to investigate\n",
    "x_me = np.array([1.0, 2.5, 2.5**2, 10, 1, 0, 10, 0]).reshape(1, -1)\n",
    "pd.DataFrame(x_me, columns=x_lab, index=['x_me'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3 # the parameter to take derivative wrt. \n",
    "\n",
    "# Let us get the beta coefficients that we are interested in.\n",
    "b_pr = probit_tab.theta.values\n",
    "b_lg = logit_tab.theta.values\n",
    "\n",
    "# Calculate the marginal effects both for the logit and probit.\n",
    "# For the probit, you can use norm.pdf for g\n",
    "# For the logit, g is straightforward algebra \n",
    "me_educ_pr = None # Fill in \n",
    "me_educ_lg = None # Fill in \n",
    "\n",
    "# print results \n",
    "pd.DataFrame([ols_results['b_hat'][k], \n",
    "              me_educ_pr[0],\n",
    "              me_educ_lg[0]],\n",
    "             index=['OLS', 'Probit', 'Logit'], columns=[f'Marg. Eff. of {x_lab[k]}']).round(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output: \n",
    "\n",
    "|        |   Marg. Eff. of educ |\n",
    "|:-------|---------------------:|\n",
    "| OLS    |             \t0.006657|\n",
    "| Probit |           \t0.007626|\n",
    "| Logit  |              0.008123|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7: Marginal effect for dummy variables \n",
    "\n",
    "Calculate the marginal effect of being a permanent foreign resident on the probability of participating in the labor market.\n",
    "\n",
    "\n",
    "For discrete variables the partial effects are given as,\n",
    "$$\n",
    "G\\left(\\beta_{0}+\\beta_{1}x_{1}+\\cdots+\\beta_{K-1}x_{K-1}+\\color{red}{\\beta_{K}} \\right)-G\\left(\\beta_{0}+\\beta_{1}x_{1}+\\cdots+\\beta_{K-1}x_{K-1}\\right)\n",
    "$$\n",
    "\n",
    "where $G$ is either $\\Phi$ for the probit model, and $\\Lambda$ for the logit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will look at the same values as previously, but we want to look at the difference for foreign = 0 and foreign = 1.\n",
    "k = 7 \n",
    "x_me2 = x_me.copy()\n",
    "x_me2[:, k] = 1  # Keep everythin the same, but change foreign to 1 for all obs. \n",
    "\n",
    "# evaluate G at the two different \"x beta\"s\n",
    "me_foreign_pr = None # FILL IN \n",
    "me_foreign_lg = None # FILL IN \n",
    "\n",
    "# print results \n",
    "pd.DataFrame([ols_results['b_hat'][k], \n",
    "              me_foreign_pr[0], # assuming me_foregin_pr is an array, otherwise remove \"[0]\"\n",
    "              me_foreign_lg[0]],\n",
    "             index=['OLS', 'Probit', 'Logit'], columns=[f'Marg. Eff.: {x_lab[k]}']).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output: \n",
    "\n",
    "|        |   Marg. Eff. of educ |\n",
    "|:-------|---------------------:|\n",
    "| OLS    |             \t0.2496|\n",
    "| Probit |           \t0.2700|\n",
    "| Logit  |              0.2726|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8: Standard errors of the marginal effects with the Delta Method\n",
    "\n",
    "(This part refers to the material covered in the note, \"The Delta Method\".) \n",
    "\n",
    "Marginal effects are a function of the estimated parameters, $\\mathbf{h}(\\hat{\\theta})$, which is a $K$-vector. The Delta Method tells us how to compute standard errors for $\\mathbf{h}(\\hat{\\boldsymbol{\\theta}})$ based on an estimated covariance matrix for $\\hat{\\boldsymbol{\\theta}}$. To do this, define the $K \\times K$ matrix of derivatives of $\\mathbf{h}$\n",
    "$$ \\mathbf{g} = \\nabla_\\theta \\mathbf{h}(\\hat{\\boldsymbol{\\theta}}).$$\n",
    "\n",
    "Then we have \n",
    "$$ \\text{Avar}[\\mathbf{h}(\\hat{\\boldsymbol{\\theta}})] = \\mathbf{g} \\, \\text{Avar}(\\hat{\\boldsymbol{\\theta}}) \\, \\mathbf{g}'$$\n",
    "\n",
    "## Probit\n",
    "\n",
    "In the following, you should compute the standard errors for the two marginal effects computed earlier, continuous (education) and discrete (foreign). \n",
    "\n",
    "### Continuous case \n",
    "From the note, we have \n",
    "\n",
    "$$ \\mathbf{g} = \\phi(\\mathbf{x}_0 \\hat{\\boldsymbol{\\beta}}) \n",
    "    \\left [\\mathbf{I}_{K\\times K} - (\\hat{\\boldsymbol{\\beta}} \\hat{\\boldsymbol{\\beta}}') (\\mathbf{x}_0' \\mathbf{x}_0) \\right] $$ \n",
    "\n",
    "Note that both $\\hat{\\boldsymbol{\\beta}} \\hat{\\boldsymbol{\\beta}}'$ and $\\mathbf{x}_0' \\mathbf{x}_0$ are $K \\times K$ matrices (outer products). \n",
    "\n",
    "***Hints:*** \n",
    "* When a vector has `ndim == 1`, transposing does nothing, so use `np.outer(a, a)` to get the outer product matrix.  \n",
    "* `np.eye(K)` gives the $K\\times K$ identity matrix, $\\mathbf{I}_{K \\times K}$. \n",
    "\n",
    "### Discrete case \n",
    "\n",
    "Here, we simply evaluate \n",
    "$$ \\mathbf{g}_k = \\phi(\\mathbf{x}_1 \\hat{\\boldsymbol{\\beta}}) \\mathbf{x}_1 - \\phi(\\mathbf{x}_0 \\hat{\\boldsymbol{\\beta}}) \\mathbf{x}_0, $$\n",
    "which is just $1 \\times K$ (as $\\mathbf{h}$), where the effect is coming only through the changes from $\\mathbf{x}_0$ to $\\mathbf{x}_1$ (which are subsumed in the notation, but we changed only the `k`th element of `x_me`.)\n",
    "\n",
    "***Hint:*** Check the output dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL IN \n",
    "bb  = None # outer product of probit parameters\n",
    "xx  = None # outer product of x_me\n",
    "I_k = None # identity matrix \n",
    "gx0 = None # g evaluated at x_me \n",
    "gx2 = None # g evaluated at x_me2\n",
    "\n",
    "# the formula (done for you)\n",
    "grad_c_pr = gx0*(I_k - bb @ xx)\n",
    "grad_d_pr = gx2*x_me2 - gx0*x_me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that you get the right dimensions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_c_pr.shape == (8,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_d_pr.shape == (1,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{Fill in} :\\quad \\text{Avar}(\\mathbf{h}) = \\mathbf{g} \\, \\text{Avar}(\\hat{\\boldsymbol{\\theta}}) \\, \\mathbf{g}' $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_se(grad, cov):\n",
    "    cov_me = None # Fill in \n",
    "    return np.sqrt(np.diag(cov_me))\n",
    "\n",
    "se_c_pr = get_se(grad_c_pr, probit_results['cov'])\n",
    "se_d_pr = get_se(grad_d_pr, probit_results['cov'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results in a nice table \n",
    "me_dict = {'Marginal Effect': np.vstack([me_educ_pr, me_foreign_pr])[:,0],\n",
    "           's.e.':            np.vstack([se_c_pr[3], se_d_pr])[:,0]}\n",
    "tab = pd.DataFrame(me_dict,index=['educ', 'foreign'])\n",
    "tab['t'] = tab['Marginal Effect'] / tab['s.e.']\n",
    "tab.index.name = 'Var'\n",
    "tab.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output: \n",
    "\n",
    "| Var     |   Marginal Effect |   s.e. |      t |\n",
    "|:--------|------------------:|-------:|-------:|\n",
    "| educ    |            0.0076 | 0.0072 | 1.0615 |\n",
    "| foreign |            0.27   | 0.0435 | 6.2068 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Question: Visual comparison\n",
    "\n",
    "Write the `predict` functions for each of the modules `LinearModel.py`, `probit_ante.py`, `logit_ante.py`, which should implement the following:  \n",
    "$$ \\begin{aligned} \n",
    "\\text{OLS} : \\hat{y}_i &= \\mathbf{x}_i \\hat{\\boldsymbol{\\beta}} \\\\ \n",
    "\\text{Probit or Logit} : \\hat{y}_i &= G(\\mathbf{x}_i \\hat{\\boldsymbol{\\beta}} ) \\\\ \n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(np.mean(lm.predict(ols_results['b_hat'],            x)), 0.4598623853211138)\n",
    "assert np.isclose(np.mean(probit.predict(probit_results['theta'], x)), 0.4606644315844275)\n",
    "assert np.isclose(np.mean(logit.predict(logit_results['theta'],   x)), 0.45986185654454603)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then just run the cell below and enjoy the sight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fit(theta, yhat, y, x, k:int, title=None): \n",
    "    '''plot_fit(): plot the predictions, f_predict(theta,x), from the model against the data\n",
    "    Args. \n",
    "        theta: array of parameters, \n",
    "        yhat: fitted values \n",
    "        y: actual outcome \n",
    "        x: regressors \n",
    "        k: index for the regressor, x[:,k], to put on the x axis \n",
    "    '''\n",
    "    \n",
    "    fig, ax = plt.subplots(); \n",
    "    ax.plot(x[:,k], yhat, 'x', alpha=0.3, label='Model'); \n",
    "    ax.plot(x[:,k], y,    'o', alpha=0.1, label='Data')\n",
    "    \n",
    "    # nicify \n",
    "    ax.set_xlabel(x_lab[k]); \n",
    "    ax.set_ylabel('Pr(y = 1|x)')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "k = 3 # which variable to put on the x-axis \n",
    "\n",
    "yhatO = lm.predict(ols_results['b_hat'], x)\n",
    "plot_fit(ols_results['b_hat'], yhatO, y, x, k, 'LPM')\n",
    "\n",
    "yhatP = probit.predict(probit_results['theta'], x)\n",
    "plot_fit(probit_results['theta'], yhatP, y, x, k, 'Probit')\n",
    "\n",
    "yhatL = logit.predict(logit_results['theta'], x)\n",
    "plot_fit(logit_results['theta'], yhatL, y, x, k, 'Logit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
